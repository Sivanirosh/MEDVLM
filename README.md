# MEDVLM
Benchmarking Foundation Models for Medical Imaging with Contextual Integration of DICOM Metadata

---

## Overview

**MEDVLM** is a research project focused on benchmarking state-of-the-art Vision-Language Models (VLMs) for medical imaging, specifically chest X-ray classification, while exploring the integration of contextual DICOM metadata (e.g., modality, patient position, acquisition time). Our goal is to evaluate whether enriching VLMs with structured clinical context can improve diagnostic performance and generalization.

---

## Project Objectives

- **Benchmark** leading Vision-Language Models (BioCLIP, GLoRIA, etc.) on chest X-ray classification.
- **Extract and preprocess** relevant DICOM metadata fields from large public datasets (CheXpert, MIMIC-CXR).
- **Integrate** DICOM metadata as structured context using Large Language Models (e.g., ClinicalBERT, BioGPT).
- **Analyze** the impact of contextual integration on model performance, robustness, and generalization.

